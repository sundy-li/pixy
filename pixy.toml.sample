# Pixy configuration sample
# Copy to ~/.pixy/pixy.toml and replace keys/models as needed.

theme = "dark"
transport_retry_count = 5
skills = ["~/.agents/skills"]

[env]
OPENAI_API_KEY = "sk-..."
ANTHROPIC_API_KEY = "sk-ant-..."

[llm]
# Use "*" to route by provider weights.
# Set to a provider `name` (for example: "openai") to pin a fixed provider.
default_provider = "*"

[[llm.providers]]
name = "openai"
kind = "chat"
provider = "openai"
api = "openai-responses"
base_url = "https://api.openai.com/v1"
api_key = "$OPENAI_API_KEY"
model = "gpt-5.3-codex"
weight = 70
reasoning = true
reasoning_effort = "high"
context_window = 200000
max_tokens = 8192

[[llm.providers]]
name = "anthropic"
kind = "chat"
provider = "anthropic"
api = "anthropic-messages"
base_url = "https://api.anthropic.com/v1"
api_key = "$ANTHROPIC_API_KEY"
model = "claude-3-5-sonnet-latest"
weight = 30

# Embedding providers can coexist in config,
# but they are ignored by coding-session chat routing.
[[llm.providers]]
name = "openai_embedding"
kind = "embedding"
provider = "openai"
api = "openai-responses"
base_url = "https://api.openai.com/v1"
api_key = "$OPENAI_API_KEY"
model = "text-embedding-3-small"
weight = 0
